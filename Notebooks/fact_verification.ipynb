{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fact_verification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIhSkgKfMiIV"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "_RHqnEZcNFV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import RobertaForSequenceClassification, BertForSequenceClassification, AlbertForSequenceClassification, DistilBertForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "hODAp1e1N7WR"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"/content/climate_train_processed.json\")\n",
        "climate_train = json.load(f)\n",
        "\n",
        "f = open(\"/content/climate_dev_processed.json\")\n",
        "climate_dev = json.load(f)\n",
        "\n",
        "f = open(\"/content/drive/MyDrive/18662/Project/SUPPORTED_claims.json\")\n",
        "supported = json.load(f)\n",
        "\n",
        "f = open(\"/content/drive/MyDrive/18662/Project/REFUTED_claims.json\")\n",
        "refuted = json.load(f)"
      ],
      "metadata": {
        "id": "__-D8vvENHDz"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for claim in refuted:\n",
        "  del claim['replace_type']\n",
        "\n",
        "generated_claims = climate_train + climate_dev + supported + refuted\n",
        "random.shuffle(generated_claims)\n"
      ],
      "metadata": {
        "id": "dPegN9nLN5Vy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for claim in generated_claims:\n",
        "  if claim['label']=='SUPPORTED':\n",
        "    claim['label'] = 1\n",
        "  if claim['label']=='REFUTED':\n",
        "    claim['label'] = 0\n",
        "  if claim['label']=='NEI':\n",
        "    claim['label'] = 2"
      ],
      "metadata": {
        "id": "ja72tl6qPdoE"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_split = int(0.8*len(generated_claims))\n",
        "train_claims = generated_claims[:train_split]\n",
        "test_claims = generated_claims[train_split:]\n"
      ],
      "metadata": {
        "id": "UBIWD1lOPNOm"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "claim_list = []\n",
        "evidence_list = []\n",
        "label_list = []\n",
        "for claim in train_claims:\n",
        "  claim_list.append(claim['claim'])\n",
        "  evidence_list.append(claim['context'])\n",
        "  label_list.append(claim['label'])\n",
        "\n",
        "train_dict = {'claim':claim_list, 'evidence':evidence_list, 'label':label_list}"
      ],
      "metadata": {
        "id": "n8Wtq22QPm27"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "claim_list = []\n",
        "evidence_list = []\n",
        "label_list = []\n",
        "for claim in test_claims:\n",
        "  claim_list.append(claim['claim'])\n",
        "  evidence_list.append(claim['context'])\n",
        "  label_list.append(claim['label'])\n",
        "\n",
        "test_dict = {'claim':claim_list, 'evidence':evidence_list, 'label':label_list}"
      ],
      "metadata": {
        "id": "7F3kbd77Prbo"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset.from_dict(train_dict)\n",
        "test_dataset = Dataset.from_dict(test_dict)"
      ],
      "metadata": {
        "id": "pf6xLoW4PtN0"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"amandakonet/climatebert-fact-checking\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(examples[\"claim\"],examples[\"evidence\"],padding=\"max_length\", truncation=True)\n",
        "\n",
        "train_tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n",
        "test_tokenized_datasets = test_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "gqikfg31PzDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokenized_datasets = train_tokenized_datasets.remove_columns([\"claim\"])\n",
        "train_tokenized_datasets = train_tokenized_datasets.remove_columns([\"evidence\"])\n",
        "test_tokenized_datasets = test_tokenized_datasets.remove_columns([\"claim\"])\n",
        "test_tokenized_datasets = test_tokenized_datasets.remove_columns([\"evidence\"])\n",
        "train_tokenized_datasets = train_tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "test_tokenized_datasets = test_tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "train_tokenized_datasets.set_format(\"torch\")\n",
        "test_tokenized_datasets.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "ZnfGmLLtQwO1"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_tokenized_datasets, shuffle=True, batch_size=8)\n",
        "eval_dataloader = DataLoader(test_tokenized_datasets, batch_size=8)"
      ],
      "metadata": {
        "id": "yS08uLr0Q0lF"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model to fine-tune here\n",
        "# <Options>\n",
        "# BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3)\n",
        "# RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=3)\n",
        "# AlbertForSequenceClassification.from_pretrained(\"albert-base-v2\", num_labels=3)\n",
        "# DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"amandakonet/climatebert-fact-checking\")"
      ],
      "metadata": {
        "id": "UChD3yMjQijV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "ieUlvVKKQuG0"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)"
      ],
      "metadata": {
        "id": "OmvRTer8RHtf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}