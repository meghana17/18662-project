{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RoBERTa-generated.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOsnT6PuchfLTdaz923oXNO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"V_MmEB6esOJ7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install sentencepiece"],"metadata":{"id":"6OySYeQ6sQ9E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install simpletransformers"],"metadata":{"id":"OjI5f5x0sTQM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install stanza"],"metadata":{"id":"s-HsNVVMsZx1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDZ3YECxq-fC"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import json, re\n","import time\n","import os\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import f1_score\n","import itertools\n","\n","from torch.utils.data import (\n","    Dataset, \n","    DataLoader,\n","    TensorDataset, \n","    random_split, \n","    RandomSampler, \n","    SequentialSampler)\n","\n","from transformers import (\n","    BertModel,\n","    BertForSequenceClassification,\n","    BertTokenizer,\n","    RobertaForSequenceClassification,\n","    RobertaTokenizer,\n","    AdamW,\n","    get_linear_schedule_with_warmup)\n"]},{"cell_type":"code","source":["batch_size = 32\n","epochs = 10\n","df_train = torch.load(\"/content/drive/MyDrive/18662/Project/Data/climate_generated_train.pt\")\n","train_dataloader = DataLoader(\n","            df_train,  \n","            batch_size = batch_size \n","        )"],"metadata":{"id":"No4-vWVttFZ6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device(\"cpu\")\n","tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\",\n","                                                           num_labels = 2,\n","                                                           output_attentions = False,\n","                                                           output_hidden_states = False\n","                                                          ).to(device)\n","\n","optimizer = AdamW(model.parameters(), lr = 5e-5, eps = 1e-8 )\n","training_stats = []\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0,num_training_steps = total_steps)"],"metadata":{"id":"wSnWK5D5wyFr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train():\n","    for epoch in range(0, epochs):\n","        print('Epoch {:} / {:}'.format(epoch + 1, epochs))\n","        train_loss = 0\n","        model.train()\n","\n","        for step, batch in enumerate(train_dataloader):\n","            input_ids = batch[0].to(device)\n","            input_mask = batch[1].to(device)\n","            labels = batch[2].to(device)\n","\n","            model.zero_grad()        \n","\n","            output = model(input_ids, token_type_ids=None, attention_mask=input_mask,labels=labels)\n","                                \n","            train_loss += output[0].item()\n","\n","            output[0].backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            optimizer.step()\n","            scheduler.step()\n","\n","        \n","        avg_train_loss = train_loss / len(train_dataloader)            \n","        \n","        print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n","        training_stats.append(\n","            {\n","                'epoch': epoch + 1,\n","                'Training Loss': avg_train_loss,\n","            }\n","        )\n","\n","    print(\"Training complete\")"],"metadata":{"id":"lXnAQoKMyAIP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["save_path = \"/content/drive/MyDrive/18662/Project/checkpoints/roberta_generated/\"\n","train()\n","model.save_pretrained(save_path)"],"metadata":{"id":"ZyNd-etVxySu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate(dev_dataloader, model):\n","    predictions = []\n","    gt = []\n","    with torch.no_grad():\n","        for step, batch in enumerate(dev_dataloader):\n","            input_ids = batch[0].to(device)\n","            input_mask = batch[1].to(device)\n","            labels = batch[2].to(device)\n","            \n","            output = model(input_ids, input_mask)\n","            predictions.append(output)   \n","            gt.append(labels)     \n","            \n","    predictions = torch.vstack([item[0].detach() for item in predictions])\n","    gt = [list(i.cpu().numpy()) for i in gt]\n","    gt = np.array(list(itertools.chain(*gt)))\n","\n","    return predictions, gt"],"metadata":{"id":"ouR-vHie3VY9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_dev = torch.load(\"/content/drive/MyDrive/18662/Project/Data/climate_generated_dev.pt\")\n","dev_dataloader = DataLoader(df_dev,  batch_size = batch_size)\n","\n","prediction, gt = evaluate(dev_dataloader, model)\n","f1 = f1_score(gt, prediction, average=None)\n","\n","print(\"F1 score for RoBERTa-large fine-tuned on CLIMATE-FEVER:\", f1)"],"metadata":{"id":"lTXXPfW34W_b"},"execution_count":null,"outputs":[]}]}